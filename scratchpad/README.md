# Scratchpad

## On Sam Altman's ['Three Observations' Blog Post](https://blog.samaltman.com/three-observations)

I stumbled across this blog post sometime in late February or early March, 2025. Despite it's short length — around 1,200 words — it sat unread in a browser tab until just now (mid-June, 2025).

Candidly, I was expecting something with a bit more _substance_.

The post opens with a brief (and extremely high-level) survey of the relationship between society and technological progress, which Altman sums up as bringing "...previously unimaginable levels of prosperity and improvements to almost every aspect of people's lives." This is certainly true, even if it hand-waves away the unequal nature in which the prosperity and improvements are doled out. He also share his belief that that AGI — Artificial _General_ Intelligence — is on the horizon.

Sam quickly turns to the technical 'meat' of the post (ie. the titular 'three observations'). These are:

1. Model performance scales logarithmically.
2. Model cost is falling rapidly.
3. Linear model improves result in 'super-exponential' impact.

I am wildly underqualified to either substantiate or refute these observations, but it's clear that the support the continued interest, emphasis, and investment into AI-related research and technology. In fairness, if we put incentives and motivation aside for a moment, Sam Altman is in perhaps the _best_ position to make these observations.

But the observations themselves are not the most interesting part of the post. Altman spends a few paragraphs describing the _impact_ of the observations (assuming that both the performance and investment trends continue). He gives the example of AI agents 'trained' as software engineers, acknowledging both that these agents my designed to approximate different levels of seniority _and_ that they will still require some type of human supervision or intervention.

The second half of the post drifts into the metaphysical. Altman points out that the effects of AGI will be so profound that we cannot truly predict them. _What_ will 'work' look like? _Who_ will work? And so on.

Altman spends a few sentences acknowledging that the power of this technology, combined with our inability to deeply understand its effects, may lead to unexpected and undesirable outcomes. In fact, he describes '...AI being used by authoritarian governments to control their population through mass surveillance and loss of autonomy.' as a 'likely' outcome. However, its clear that Altman believes that the potential upside more than justifies this possibility.

Most evocative is Altman's claim that 'Anyone in 2035 should be able to marshall the intellectual capacity equivalent to everyone in 2025...'. Here, I take his use of 'everyone' rather than 'anyone' to be deliberate. Altman is not saying that any old person in 2035 will have access to the intellectual capacity of _some specific person_ from 2025, he's saying that they'll have access to the total intellectual capacity of _every single person_ from 2025. Given the progress that's being made _in_ 2025, that's quite the statement.

So what's my take on all of this?

Well, as I said above, I can't speak to any of the 'technical bits' (as vague as they may be). I agree with Altman that the effects of AI in general — and AGI in particular — will be profound and disruptive. I agree that many existing knowledge work roles will be either entirely or primarily 'outsourced' to AI, and that entirely new roles will emerge. Whether this narrows or widens existing intra and international class divides is unclear.

It seems safe to say that AI is here to stay, and that AI-related and AI-adjacent roles will remain in high demand (at least for the near term). Altman includes a sentences to the effect of AI as a tool for unlocking human potential and creativity. I certainly hope that's true.